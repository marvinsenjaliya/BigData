{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///home/hduser/export_dataframe.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "textfile = sc.textFile(\"file:///home/hduser/export_dataframe.csv\")\n",
    "print(textfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|            username|          idle_time|       working_hour|         start_time|           End_time|\n",
      "+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  sahil24c@gmail.com|2019-10-24 12:40:00|2019-10-24 23:55:00|2019-10-24 05:30:01|2019-10-25 05:25:01|\n",
      "|  yathink3@gmail.com|2019-10-24 12:30:00|2019-10-24 23:55:01|2019-10-24 05:30:01|2019-10-25 05:25:02|\n",
      "|magadum.iranna@gm...|2019-10-24 13:35:00|2019-10-24 23:55:01|2019-10-24 05:30:01|2019-10-25 05:25:02|\n",
      "|  shelkeva@gmail.com|2019-10-24 00:40:00|2019-10-24 09:40:01|2019-10-24 08:45:01|2019-10-24 18:25:02|\n",
      "|puruissimple@gmai...|2019-10-24 01:50:00|2019-10-24 10:49:59|2019-10-24 08:50:02|2019-10-24 19:40:01|\n",
      "|sangita.awaghad19...|2019-10-24 01:05:00|2019-10-24 10:35:00|2019-10-24 08:50:01|2019-10-24 19:25:01|\n",
      "|vaishusawant143@g...|2019-10-24 00:05:00|2019-10-24 10:50:01|2019-10-24 08:55:01|2019-10-24 19:45:02|\n",
      "|     you@example.com|2019-10-24 00:25:00|2019-10-24 10:40:00|2019-10-24 08:55:01|2019-10-24 19:35:01|\n",
      "|samadhanmahajan73...|2019-10-24 00:10:00|2019-10-24 09:49:59|2019-10-24 09:00:02|2019-10-24 18:50:01|\n",
      "|vishnu23kumar@gma...|2019-10-24 00:00:00|2019-10-24 10:55:01|2019-10-24 09:00:01|2019-10-24 19:55:02|\n",
      "|ashutoshrit64@gma...|2019-10-24 00:35:00|2019-10-24 12:00:00|2019-10-24 09:00:01|2019-10-24 21:00:01|\n",
      "|akshaybavalekar10...|2019-10-24 00:25:00|2019-10-24 10:20:00|2019-10-24 09:05:01|2019-10-24 19:25:01|\n",
      "|khairnarswapna99@...|2019-10-24 00:40:00|2019-10-24 11:30:00|2019-10-24 09:10:01|2019-10-24 20:40:01|\n",
      "|kukadeshilpaa7m95...|2019-10-24 00:00:00|2019-10-24 10:20:00|2019-10-24 09:10:01|2019-10-24 19:30:01|\n",
      "|sarikabarge111@gm...|2019-10-24 00:05:00|2019-10-24 10:05:00|2019-10-24 09:10:01|2019-10-24 19:15:01|\n",
      "|narsimharaj.kasu0...|2019-10-24 00:05:00|2019-10-24 10:10:00|2019-10-24 09:10:01|2019-10-24 19:20:01|\n",
      "|antonyalexcm@gmai...|2019-10-24 00:55:00|2019-10-24 10:25:01|2019-10-24 09:10:01|2019-10-24 19:35:02|\n",
      "|jitupatil937@gmai...|2019-10-24 00:55:00|2019-10-24 10:25:00|2019-10-24 09:10:01|2019-10-24 19:35:01|\n",
      "|akshaypatwari24@g...|2019-10-24 00:30:00|2019-10-24 10:15:00|2019-10-24 09:10:01|2019-10-24 19:25:01|\n",
      "|aheteshams007@gma...|2019-10-24 00:20:00|2019-10-24 10:50:00|2019-10-24 09:10:01|2019-10-24 20:00:01|\n",
      "+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- username: string (nullable = true)\n",
      " |-- idle_time: string (nullable = true)\n",
      " |-- working_hour: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- End_time: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"file:///home/hduser/export_dataframe.csv\" , inferSchema=True , header=True)\n",
    "print(df.show())\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe('working_hour').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "df.select('username','working_hour').show()\n",
    "df.select(mean(df(\"working_hour\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_commers = df.filter(df.idle_time>=\"2019-10-24 09:30:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(mean(df(\"working_hour\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(mean(\"working_hour\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(mean['working_hour']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36044\n",
      "+--------------------+-------+\n",
      "|            username|seconds|\n",
      "+--------------------+-------+\n",
      "|  sahil24c@gmail.com|  86100|\n",
      "|  yathink3@gmail.com|  86101|\n",
      "|magadum.iranna@gm...|  86101|\n",
      "|puruissimple@gmai...|  38999|\n",
      "|sangita.awaghad19...|  38100|\n",
      "|vaishusawant143@g...|  39001|\n",
      "|     you@example.com|  38400|\n",
      "|vishnu23kumar@gma...|  39301|\n",
      "|ashutoshrit64@gma...|  43200|\n",
      "|akshaybavalekar10...|  37200|\n",
      "|khairnarswapna99@...|  41400|\n",
      "|kukadeshilpaa7m95...|  37200|\n",
      "|sarikabarge111@gm...|  36300|\n",
      "|narsimharaj.kasu0...|  36600|\n",
      "|antonyalexcm@gmai...|  37501|\n",
      "|jitupatil937@gmai...|  37500|\n",
      "|akshaypatwari24@g...|  36900|\n",
      "|aheteshams007@gma...|  39000|\n",
      "|sargampandey27oct...|  36899|\n",
      "|ayush.saraf47@gma...|  36601|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.groupBy('username').agg({'idle_time': 'mean'}).show()\n",
    "import datetime\n",
    "from pyspark.sql.functions import col, lit, avg \n",
    "new_working_hour = []\n",
    "converted_seconds_sum = 0\n",
    "for row in df.collect():\n",
    "    date_time = datetime.datetime.strptime(row['working_hour'], \"%Y-%m-%d %H:%M:%S\")\n",
    "    converted_seconds = date_time.hour * 3600 + date_time.minute *60 + date_time.second\n",
    "    new_working_hour.append((row['username'], converted_seconds))\n",
    "    converted_seconds_sum = converted_seconds_sum + converted_seconds\n",
    "    #print(new_working_hour) \n",
    "avg_seconds = converted_seconds_sum/88\n",
    "print(avg_seconds)\n",
    "new_column = sqlContext.createDataFrame(new_working_hour, ('username',\"seconds\"))\n",
    "#new_column.filter(new_column['seconds'] > avg_seconds).show()\n",
    "#joined_dataframe = (new_column.join(new_column, col(\"username\")==col(\"username\"),\"leftouter\").drop(\"username\"))\n",
    "\n",
    "\n",
    "new_column.filter(new_column.seconds >= avg_seconds).show()\n",
    "\n",
    "#average_object = new_column.agg(avg(col(\"seconds\"))\n",
    "#average_object = new_column.agg(avg(col(\"seconds\"))).show()\n",
    "#new_column.filter(new_column['seconds'] > ).agg(avg(col(\"seconds\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
